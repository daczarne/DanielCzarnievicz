---
date: '2020-04-13'
draft: false
title: "Notas sobre muestreo"
weight: 0
output: blogdown::html_page
---



<p>Resumenes de los cursos de muestreo del Instituto de Estadística (IESTA - FCEA - Udelar).</p>
<!--more-->
<p>Aquí podrás encontrar algunos resumenes sobre las técnicas de muestreo cubiertas en los cursos de muestreo del IESTA. El material que aquí se presenta está basado en el libro <em>Model Assisted Survey Sampling</em> de Särndal, Swensson y Wretman.</p>
<div id="Nociones-básicas-sobre-muestreo-de-poblaciones-finitas" class="section level2">
<h2>Nociones básicas sobre muestreo de poblaciones finitas</h2>
<div id="muestreo-sin-reposición" class="section level3">
<h3>Muestreo sin reposición</h3>
<p>Sea la poblacón <span class="math inline">\(U = \{1; \ldots; k; \ldots; N \}\)</span> donde <span class="math inline">\(N\)</span> es conocido, pero los valores de <span class="math inline">\(y_k\)</span> son desconocidos. Se busca estimar el total de la variable <span class="math inline">\(y\)</span>, <span class="math inline">\(t_y = \sum\nolimits_U y_k\)</span>, o de la media poblacional <span class="math inline">\(\bar{y}_U = \frac{t_y}{N} = \frac{1}{N} \sum\nolimits_U y_k\)</span>. Se selecciona una <em>muestra</em> de la población, la cual se utiliza para estimar el total y la media.</p>
<p>Llamamos <em>diseño muestral</em> a la función <span class="math inline">\(p(.)\)</span> tal que:
<span class="math display">\[{\color{red} \star} \: \: p(s) = \Pr (S=s) = \Pr ( \text{seleccionar la muestra } s \, | \, \text{estrategia de selección})\]</span>
<span class="math inline">\(p(s)\)</span> es entonces la función de distribución de probabilidad de una variable aleatoria <span class="math inline">\(S\)</span> con recorrido <span class="math inline">\(\mathscr{S} = \{s_1; s_2; \ldots \}\)</span>. <span class="math inline">\(\mathscr{S}\)</span> tiene <span class="math inline">\(2^N\)</span> elementos, contando <span class="math inline">\(\emptyset\)</span> y <span class="math inline">\(U\)</span>. Dado que es una función de probabilidad, <span class="math inline">\(p(s) \geq 0 \: \forall s \in \mathscr{S}\)</span>, y <span class="math inline">\(\sum\limits_{ s \in \mathscr{S}} p(s) = 1\)</span>.</p>
<p>La inclusión de un elemento <span class="math inline">\(k\)</span> en la muestra puede indicarse mediante la indicadora:
<span class="math display">\[{\color{red} \star} \: \: I_k = \left\{
\begin{array}{c c c}
1 &amp; \text{si} &amp; k \in s \\
0 &amp; \text{si} &amp; k \notin s
\end{array} \right.\]</span></p>
<p>Definimos la <em>probabilidad de inclusión de primer orden</em> como:
<span class="math display">\[{\color{red} \star } \: \: \pi_k = \Pr (k \in s) = \Pr (I_k = 1) = \sum\limits_{s \ni k} p(s)\]</span>
Existen <span class="math inline">\(N\)</span> cantidades <span class="math inline">\(\pi_1; \ldots; \pi_k; \ldots; \pi_N\)</span> asociadas a un diseño <span class="math inline">\(p(.)\)</span>. Si <span class="math inline">\(\pi_k \geq 0 \; \forall k \in U\)</span> decimos que el muestreo es un <em>muestreo probabilístico</em>.</p>
<p>Definimos la <em>probabilidad de inclusión de segundo orden</em> como:
<span class="math display">\[{\color{red} \star } \: \: \pi_{kl} = \Pr (k;l \in s) = \Pr (I_k I_l = 1) = \sum\limits_{\substack{ s \ni k \\ s \ni l}} p(s)\]</span>
Existen <span class="math inline">\(\frac{N(N-1)}{2}\)</span> cantidades <span class="math inline">\(\pi_{12}; \pi_{13}; \ldots; \pi_{kl}; \ldots; \pi_{N-1,N}\)</span> asociadas a un diseño <span class="math inline">\(p(.)\)</span>, donde <span class="math inline">\(\pi_{kl} = \pi_{lk}\)</span> y <span class="math inline">\(\pi_{kk} = \pi_k\)</span>. Si <span class="math inline">\(\pi_{kl} \geq 0 \; \forall k,l \in U\)</span> decimos que el diseño es <em>medible</em>. Solo en el caso de diseños medibles es posible obtener estimadores insesgados de la varianza.</p>
<p>Para todo diseño se cumple que:</p>
<p><span class="math inline">\({\color{red} \star} \: \: {\E}_{p(s)}(I_k) = \pi_k \; \; \; \forall k \in U\)</span></p>
<p><span class="math inline">\({\color{red} \star} \: \: {\V}_{p(s)}(I_k) = \pi_k (1 - \pi_k) \; \; \; \forall k \in U\)</span></p>
<p><span class="math inline">\({\color{red} \star} \: \: \Delta_{kl} = {\C}_{p(s)}(I_k;I_l) = \pi_{kl} - \pi_k \, \pi_l \; \; \; \forall k \neq l \in U\)</span></p>
<p>Llamamos <span class="math inline">\(n_S\)</span> al tamaño muestral (es decir, al cardinal del conjunto <span class="math inline">\(s\)</span>). Para todo diseño se cumple que:</p>
<p><span class="math inline">\({\color{red} \star} \: \: n_S = \sum\nolimits_U I_k\)</span></p>
<p><span class="math inline">\({\color{red} \star} \: \: {\E}_{p(s)}(n_S) = \sum\nolimits_U \pi_k\)</span></p>
<p><span class="math inline">\({\color{red} \star} \: \: {\V}_{p(s)}(n_S) = \sum\nolimits_U \pi_k(1-\pi_k) + \mathop{\sum\sum\nolimits_U}_{\!\!\!\!\!\! k \neq l} (\pi_{kl} - \pi_k \, \pi_l) = \sum\nolimits_U \pi_k - \left( \sum\nolimits_U \pi_k \right)^2 + \mathop{\sum\sum\nolimits_U}_{\!\!\!\!\!\! k \neq l} \pi_{kl}\)</span></p>
<p>Adicionalmente, si <span class="math inline">\(p(s)\)</span> es de tamaño fijo <span class="math inline">\(n\)</span>, entonces:</p>
<p><span class="math inline">\({\color{red} \star} \: \: {\mathbf{E}}_{p(s)}(n_S) = \sum\nolimits_U \pi_k = n\)</span></p>
<p><span class="math inline">\({\color{red} \star} \: \: \mathop{\sum\sum\nolimits_U}_{\!\!\!\!\!\! k \neq l} \pi_{kl} = n(n-1)\)</span></p>
<p><span class="math inline">\({\color{red} \star} \: \: \sum\limits_{\substack{ l \in U \\ l \neq k}} \pi_{kl} = \sum\limits_{\substack{ l \in U \\ l \neq k}} {\E}_{p(s)}(I_k I_l) = {\E}_{p(s)} \Big[ I_k \left( \sum\nolimits_U I_l - I_k \right) \Big] = {\E}_{p(s)} \Big( I_k \underbrace{\sum\nolimits_U I_l}_{=n} \Big) - {\E}_{p(s)}(I_l^2) =\)</span></p>
<p><span class="math display">\[= {\E}_{p(s)}(I_k n) - {\E}_{p(s)}(I_k) = n \, {\E}_{p(s)}(I_k) - {\E}_{p(s)}(I_k) = n \, \pi_k - \pi_k = (n - 1) \pi_k\]</span></p>
</div>
<div id="el-estimador-hatt_pi" class="section level3">
<h3>El estimador <span class="math inline">\(\hat{t}_{\pi}\)</span></h3>
<p>El principio de <span class="math inline">\(\pi\)</span>-expansión implica que el elemento muestral <span class="math inline">\(k\)</span> representa <span class="math inline">\(^1\!/_{\pi_k}\)</span> elementos en la población. Sea el siguiente estimador de <span class="math inline">\(t_y\)</span>:
<span class="math display">\[{\color{red} \star} \: \: \hat{t}_{\pi} = \sum\nolimits_s y_k^{\checkmark} = \sum\nolimits_s \frac{y_k}{\pi_k}\]</span>
<span class="math display">\[{\color{red} \star} \: \: {\E}_{p(s)}(\hat{t}_{\pi}) = {\E}_{p(s)} \left( \sum\nolimits_s \frac{y_k}{\pi_k} \right) = \sum\nolimits_U {\E}_{p(s)}(I_k) \, \frac{y_k}{\pi_k} = \sum\nolimits_U \pi_k \, \frac{y_k}{\pi_k} = \sum\nolimits_U y_k = t_y\]</span>
<span class="math display">\[{\color{red} \star} \: \: {\V}_{p(s)}(\hat{t}_{\pi}) = {\V}_{p(s)} \left( \sum\nolimits_s y_k^{\checkmark} \right) = \sum\nolimits_U {\V}_{p(s)}(I_k) \, y_k^{\checkmark^2} + \mathop{\sum\sum\nolimits_U}_{\!\!\!\!\!\! k \neq l} {\C}_{p(s)}(I_k;I_l) \, y_k^{\checkmark} \, y_l^{\checkmark} =\]</span>
<span class="math display">\[= \sum\nolimits_U \Delta_{kk} \, y_k^{\checkmark} \, y_k^{\checkmark} + \mathop{\sum\sum\nolimits_U}_{\!\!\!\!\!\! k \neq l} \Delta_{kl} \, y_k^{\checkmark} \, y_l^{\checkmark} = \sum\sum\nolimits_U \Delta_{kl} \, y_k^{\checkmark} \, y_l^{\checkmark}\]</span>
<span class="math display">\[{\color{red} \star} \: \: \hat{\V}_{p(s)}(\hat{t}_{\pi}) = \sum\sum\nolimits_s \Delta_{kl}^{\checkmark} \, y_k^{\checkmark} \, y_l^{\checkmark}\]</span>
<span class="math display">\[{\color{red} \star} \: \: {\E}_{p(s)} \Big( \hat{\V}_{p(s)}(\hat{t}_{\pi}) \Big) = {\E}_{p(s)} \left( \sum\sum\nolimits_s \Delta_{kl}^{\checkmark} \, y_k^{\checkmark} \, y_l^{\checkmark} \right) = \sum\sum\nolimits_U {\E}_{p(s)}(I_k;I_l) \Delta_{kl}^{\checkmark} \, y_k^{\checkmark} \, y_l^{\checkmark} =\]</span>
<span class="math display">\[= \sum\sum\nolimits_U \pi_{kl} \, \frac{\Delta_{kl}}{\pi_{kl}} \, y_k^{\checkmark} \, y_l^{\checkmark} = \sum\sum\nolimits_U \Delta_{kl} \, y_k^{\checkmark} \, y_l^{\checkmark} = {\V}_{p(s)}(\hat{t}_{\pi}) \]</span></p>
<p>Si el diseño es de tamaño fijo, son válidas las siguientes expresiones:
<span class="math display">\[{\color{red} \star} \: \: {\V}_{p(s)}(\hat{t}_{\pi}) = - \frac{1}{2} \sum\sum\nolimits_U \Delta_{kl} \big( y_k^{\checkmark} - y_l^{\checkmark} \big)^2\]</span>
<span class="math display">\[{\color{red} \star} \: \: \hat{\V}_{p(s)}(\hat{t}_{\pi}) = - \frac{1}{2} \sum\sum\nolimits_s \Delta_{kl}^{\checkmark} \big( y_k^{\checkmark} - y_l^{\checkmark} \big)^2\]</span>
<span class="math display">\[\begin{array}{rcl}
{\color{red} \star} \: \: {\E}_{p(s)} \Big( \hat{\V}_{p(s)}(\hat{t}_{\pi}) \Big) &amp; = &amp; {\E}_{p(s)} \left( - \frac{1}{2} \sum\sum\nolimits_s \Delta_{kl}^{\checkmark} \big( y_k^{\checkmark} - y_l^{\checkmark} \big)^2 \right) \\ \\
   &amp; = &amp; - \frac{1}{2} \sum\sum\nolimits_U {\E}_{p(s)} \left( I_k; I_l \right) \Delta_{kl}^{\checkmark} \big( y_k^{\checkmark} - y_l^{\checkmark} \big)^2 \\ \\
   &amp; = &amp; - \frac{1}{2} \sum\sum\nolimits_U {\E}_{p(s)} ( I_k; I_l ) \Delta_{kl}^{\checkmark} \big( y_k^{\checkmark} - y_l^{\checkmark} \big)^2 \\ \\
   &amp; = &amp; - \frac{1}{2} \sum\sum\nolimits_U \Delta_{kl} \big( y_k^{\checkmark} - y_l^{\checkmark} \big)^2 \\ \\
   &amp; = &amp; {\V}_{p(s)}(\hat{t}_{\pi})
\end{array}\]</span></p>
<p><em>Demostración</em>:</p>
<p><span class="math display">\[\begin{array}{rcl}
{\V}_{p(s)}(\hat{t}_{\pi}) &amp; = &amp; - \frac{1}{2} \sum\sum\nolimits_U \Delta_{kl} \big( y_k^{\checkmark} - y_l^{\checkmark} \big)^2 \\ \\
   &amp; = &amp; - \frac{1}{2} \sum\sum\nolimits_U \Delta_{kl} \big( y_k^{\checkmark^2} - 2 \, y_k^{\checkmark} \, y_l^{\checkmark} + y_l^{\checkmark^2} \big) \\ \\
   &amp; = &amp; - \frac{1}{2} \sum\sum\nolimits_U \Delta_{kl} \, y_k^{\checkmark^2} + \sum\sum\nolimits_U \Delta_{kl} \, y_k^{\checkmark} \, y_l^{\checkmark} - \frac{1}{2} \sum\sum\nolimits_U \Delta_{kl} \, y_l^{\checkmark^2} \\ \\
   &amp; = &amp; \sum\sum\nolimits_U \Delta_{kl} \, y_k^{\checkmark} \, y_l^{\checkmark} - \sum\sum\nolimits_U \Delta_{kl} \, y_k^{\checkmark^2} \\ \\
   &amp; = &amp; \sum\sum\nolimits_U \Delta_{kl} \, y_k^{\checkmark} \, y_l^{\checkmark} - \sum\limits_{k \in U} y_k^{\checkmark^2} \sum\limits_{l \in U} \Delta_{kl} \\ \\
   &amp; = &amp; \sum\sum\nolimits_U \Delta_{kl} \, y_k^{\checkmark} \, y_l^{\checkmark} - \sum\limits_{k \in U} y_k^{\checkmark^2} \sum\limits_{l \in U} \big( \pi_{kl} - \pi_k \, \pi_l \big) \\ \\
   &amp; = &amp; \sum\sum\nolimits_U \Delta_{kl} \, y_k^{\checkmark} \, y_l^{\checkmark} - \sum\limits_{k \in U} y_k^{\checkmark^2} \left[ \sum\limits_{l \in U} \pi_{kl} - \sum\limits_{l \in U} \pi_k \, \pi_l \right] \\ \\
   &amp; = &amp; \sum\sum\nolimits_U \Delta_{kl} \, y_k^{\checkmark} \, y_l^{\checkmark} - \sum\limits_{k \in U} y_k^{\checkmark^2} \left[ n \, \pi_k - \pi_k \, \sum\limits_{l \in U} \pi_l \right] \\ \\
   &amp; = &amp; \sum\sum\nolimits_U \Delta_{kl} \, y_k^{\checkmark} \, y_l^{\checkmark} - \sum\limits_{k \in U} y_k^{\checkmark^2} \big( n \, \pi_k - \pi_k \, n \big) \\ \\
   &amp; = &amp; \sum\sum\nolimits_U \Delta_{kl} \, y_k^{\checkmark} \, y_l^{\checkmark}
\end{array}\]</span></p>
</div>
<div id="muestreo-con-reposición" class="section level3">
<h3>Muestreo con reposición</h3>
<p>Llamamos muestreo (no diseño) con reposición a los esquemas en los que los elementos son repuestos en la población luego de ser seleccionados. Por tanto, dos (o más) extracciones podrían producir el mismo elemento. Llamamos <span class="math inline">\(k_i\)</span> al elemento seleccionado en la <span class="math inline">\(i\)</span>-ésima extracción con <span class="math inline">\(i=1;\ldots;m\)</span>. Al vector que contiene todos los elementos seleccionados lo llamamos <em>muestra ordenada</em>: <span class="math inline">\(os = \{k_1; \ldots; k_m \}\)</span>. Llamamos <em>multiplicidad</em> a la cantidad de veces que un mismo elemento fue seleccionado. Toda muestra ordenada, <span class="math inline">\(os\)</span>, induce una muestra, <span class="math inline">\(s\)</span>, la cual contiene a los elementos sorteados una única vez (se pierde el orden).
<span class="math display">\[s=\{k: k = k_i \text{ para alguna extracción } i=1;\ldots;m \}\]</span></p>
<p>Sean <span class="math inline">\(p_1; \ldots; p_k; \ldots; p_N\)</span> números positivos tales que <span class="math inline">\(\sum\nolimits_U p_k = 1\)</span>. Dado que los elementos se reponen en la población una vez seleccionado:
<span class="math display">\[p_k = \Pr(\text{seleccionar el elemento } k \text{ en la } i \text{-ésima extracción}) \; \; \forall k \in U\]</span></p>
<p>De esta forma, la probabilidad de obtener una determinada muestra ordenada será:
<span class="math display">\[p(os) = \Pr \big(os = \{k_1; \ldots; k_m \} \big) = p_{k_1} \times \ldots \times p_{k_m} = \prod\limits_{i=1}^{m} p_{k_i} \]</span></p>
<p>Sea la variable aleatoria <span class="math inline">\(r_k\)</span>, la cual mide la cantidad de veces que el elemento <span class="math inline">\(k\)</span> es extraído en las <span class="math inline">\(m\)</span> extracciones. Por lo tanto, <span class="math inline">\(r_k \sim Bin(m; p_k)\)</span>. Si <span class="math inline">\(N\)</span> es lo suficientemente grande, <span class="math inline">\(r_k \overset{a}{\sim} \text{Poisson}(m \, p_k)\)</span>.</p>
<p><span class="math display">\[{\color{red} \star} \: \: \Pr(\text{extraer } r_0 \text{ veces el elemento } k ) = \Pr (r_k = r_0) = {{m}\choose{r}} (p_k)^r ( 1 - p_k )^{m-r}\]</span></p>
<p><span class="math display">\[{\color{red} \star} \: \: {\E}(r_k) = m \, p_k\]</span></p>
<p><span class="math display">\[\color{red} \star \color{black} \: \: {\V}(r_k) = m \, p_k (1 - p_k) \doteq m \, p_k\]</span></p>
<p>La probabilidad de que el elemento <span class="math inline">\(k\)</span> nunca sea seleccionado en las <span class="math inline">\(m\)</span> extracciones, y la probabilidad de que el elemento <span class="math inline">\(k\)</span> sea seleccionado al menos una vez en las <span class="math inline">\(m\)</span> extracciones son:</p>
<p><span class="math display">\[{\color{red} \star} \: \: \Pr (\text{no selecionar } k \text{ en ninguna de las } m \text{ extraciciones}) = \Pr (r_k = 0) = (1 - p_k)^m\]</span></p>
<p><span class="math display">\[{\color{red} \star} \: \: \Pr (\text{el elemento } k \text{ sea extraido}) = \Pr (r_k \geq 1) = 1 - \Pr (r_k &lt; 1) = 1 - \Pr (r_k = 0) = 1 - (1 - p_k)^m\]</span></p>
<p>Por lo tanto, las probabilidades de inclusión de primer y segundo orden serán:</p>
<p><span class="math display">\[{\color{red} \star} \: \: \pi_k = \Pr (k \in S) = 1 - (1 - p_k)^m\]</span></p>
<p><span class="math display">\[{\color{red} \star} \: \: \pi_{kl} = \Pr (k;l \in S) = \Pr (k \in S) \, \Pr (l \in S) = \big[ 1 - (1-p_k)^m \big] \big[ 1 - (1-p_l)^m \big] =\]</span>
<span class="math display">\[= 1 - (1-p_l)^m - (1-p_k)^m + (1-p_k)^m (1-p_l)^m =\]</span>
<span class="math display">\[= 1 - (1-p_k)^m - (1-p_l)^m + \big[ (1-p_k) (1-p_l) \big]^m =\]</span>
<span class="math display">\[= 1 - \big[ (1-p_k)^m + (1-p_l)^m - (1 - p_k - p_l + p_k \, p_l)^m \big] \: \: \forall k \neq l \in U\]</span></p>
<p>Si <span class="math inline">\(p_k = p_l \: \: \forall k;l \in U\)</span></p>
<p><span class="math display">\[{\color{red} \star} \: \: \pi_{kl} = 1 - \big[ 2(1 - p_k)^m - (1 - 2p_k + p_k^2)^m \big] =\]</span>
<span class="math display">\[= 1 - \big[ 2(1 - p_k)^m - (1 - p_k)^{2m} \big]  \: \: \forall k \neq l \in U\]</span></p>
</div>
<div id="el-estimador-hatt_pwr" class="section level3">
<h3>El estimador <span class="math inline">\(\hat{t}_{pwr}\)</span></h3>
<p>En muestreos con reposición <span class="math inline">\(t_y\)</span> se estima utilizando un estimador <span class="math inline">\(p\)</span>-expandido (en lugar del estimador <span class="math inline">\(\pi\)</span>-expandido):
<span class="math display">\[{\color{red} \star} \: \: \hat{t}_{pwr} = \frac{1}{m} \sum\limits_{i=1}^{m} \frac{y_k}{p_k}\]</span></p>
<p>Para hallar las propiedades estadísticas del estimador <span class="math inline">\(\hat{t}_{pwr}\)</span> se definen las variables <span class="math inline">\(Z_i = \frac{y_{k_i}}{p_{k_i}}\)</span>. De esta forma, <span class="math inline">\(\hat{t}_{pwr} = \frac{1}{m} \sum\limits_{i=1}^{m} Z_i = \bar{Z}\)</span>. Dado que las <span class="math inline">\(Z_i\)</span> son iid:
<span class="math display">\[{\color{red} \star} \: \: \Pr \left( Z_k = \frac{y_k}{p_k} \right) = p_k \: \: \: \forall i=1;\ldots;m\]</span>
<span class="math display">\[{\color{red} \star} \: \: {\E}(Z_i) = \sum\nolimits_U Z_i \, \Pr \left( Z_k = \frac{y_k}{p_k} \right) = \sum\nolimits_U \frac{y_k}{p_k} \, \Pr \left( Z_k = \frac{y_k}{p_k} \right) = \sum\nolimits_U \frac{y_k}{p_k} \, p_k = \sum\nolimits_U y_k = t_y\]</span>
<span class="math display">\[{\color{red} \star} \: \: \V(Z_i) = \E \Big[ (Z_i - t_y )^2 \Big] = \E(Z_i^2) - t_y^2 = \sum\nolimits_U \frac{y_k^2}{p_k} - t_y^2 = \sum\nolimits_U \frac{y_k^2}{p_k} - 2 \, t_y^2 + t_y^2 =\]</span>
<span class="math display">\[= \sum\nolimits_U \frac{y_k^2}{p_k} - 2 \, t_y \sum\nolimits_U y_k + t_y^2 \sum\nolimits_U p_k = \sum\nolimits_U \left[ \frac{y_k^2}{p_k} - 2 \, t_y \, y_k + t_y^2 \, p_k \right] =\]</span>
<span class="math display">\[= \sum\nolimits_U \left[ \left( \frac{y_k}{p_k} \right)^2 - 2 \, t_y \left( \frac{y_k}{p_k} \right) + t_y^2 \right] p_k = \sum\nolimits_U \left[ \left( \frac{y_k}{p_k} - t_y \right)^2 p_k \right] = V_i\]</span>
<span class="math display">\[{\color{red} \star} \: \: {\E}_{p(os)} \big( \hat{t}_{pwr} \big) = \E \big( \bar{Z} \big) = \E \left( \frac{1}{m} \sum\limits_{i=1}^{m} z_i \right) = \frac{1}{m} \sum\limits_{i=1}^{m} \E(Z_i) = \frac{1}{m} (m, \, t_y) = t_y \]</span>
<span class="math display">\[{\color{red} \star} \: \: {\V}_{p(os)} \big( \hat{t}_{pwr} \big) = {\V} \big( \bar{Z} \big) = \frac{1}{m} {\V} (Z_i) = \frac{1}{m} \sum\nolimits_U \left[ \left( \frac{y_k}{p_k} - t_y \right)^2 p_k \right] = \frac{V_i}{m}\]</span>
<span class="math display">\[{\color{red} \star} \: \: \hat{V}_i = \frac{1}{m - 1} \sum\limits_{i=1}^{m} \left( \frac{y_k}{p_k} - \hat{t}_{pwr} \right)^2\]</span>
<span class="math display">\[{\color{red} \star} \: \: \hat{\V}_{p(os)} \big( \hat{t}_{pwr} \big) = \frac{\hat{V}_i}{m} = \frac{1}{m(m-1)} \sum\limits_{i=1}^{m} \left( \frac{y_k}{p_k} - \hat{t}_{pwr} \right)^2\]</span>
<span class="math display">\[{\color{red} \star} \: \: {\E}_{p(os)} \Big( \hat{\V}_{p(os)} \big( \hat{t}_{pwr} \big) \Big) = {\E}_{p(os)} \Big( \hat{\V}(\bar{Z}) \Big) = \frac{1}{m} \, {\E}_{p(os)} \Big( \hat{\V}(Z_i) \Big) = \frac{1}{m} \, {\V}(Z_i) = \frac{V_i}{m} \Leftrightarrow\]</span>
<span class="math display">\[\Leftrightarrow \hat{V}_i = \frac{1}{m-1} \sum\limits_{i=1}^{m} \big( Z_i - \bar{Z} \big)^2 = \frac{1}{m-1} \sum\limits_{i=1}^{m} \left( \frac{y_k}{p_k} - \hat{t}_{pwr} \right)^2\]</span>
<span class="math display">\[\therefore \hat{\V}_{p(os)} \big( \hat{t}_{pwr} \big) \text{ es insesgada para } {\V}_{p(os)} \big( \hat{t}_{pwr} \big)\]</span></p>
<p><span class="math display">\[\text{Si } y_k = c \, p_k \: \: \forall k \in U \Rightarrow t_y = \sum\nolimits_U y_k = \sum\nolimits_U c \, p_k = c \underbrace{\sum\nolimits_U p_k}_{=1} = c \Rightarrow\]</span>
<span class="math display">\[\Rightarrow {\V}_{p(os)} \big( \hat{t}_{pwr} \big) = \frac{1}{m} \sum\nolimits_U \left( \frac{y_k}{p_k} - t_y \right)^2 p_k = \frac{1}{m} \sum\nolimits_U \left( \frac{cp_k}{p_k} - c \right)^2 p_k = \frac{1}{m} \sum\nolimits_U \left( c - c \right)^2 p_k = 0\]</span></p>
<p>En la práctica no es posible establecer <span class="math inline">\(y_k = c \, p_k\)</span>. Pero sí es posible establecer <span class="math inline">\(p_k = \frac{x_k}{\sum\nolimits_U x_k} \: \: \forall k \in U\)</span>, donde <span class="math inline">\(x_k\)</span> es una variable auxiliar (es decir, <span class="math inline">\(x_k\)</span> y <span class="math inline">\(y_k\)</span> están altamente correlacionadas).</p>
</div>
<div id="el-estimador-hatt_pi-en-el-diseño-ordenado" class="section level3">
<h3>El estimador <span class="math inline">\(\hat{t}_{\pi}\)</span> en el diseño ordenado</h3>
<p><span class="math display">\[{\color{red} \star } \: \: \hat{t}_{\pi} = \sum\nolimits_s y_k^{\checkmark} = \sum\nolimits_s \frac{y_k}{\pi_k} = \sum\nolimits_s \frac{y_k}{1 - (1 - p_k)^m}\]</span></p>
<p>El cual es insesgado para <span class="math inline">\(t_y\)</span>, y la expresión para su varianza y el estimador de su varianza son:
<span class="math display">\[{\color{red} \star } \: \: {\V}_{p(s)} \big( \hat{t}_{\pi} \big) = \sum\sum\nolimits_U \Delta_{kl} \, y_k^{\checkmark} \, y_l^{\checkmark}\]</span>
<span class="math display">\[{\color{red} \star } \: \: \hat{\V}_{p(s)} \big( \hat{t}_{\pi} \big) = \sum\sum\nolimits_s \Delta_{kl}^{\checkmark} \, y_k^{\checkmark} \, y_l^{\checkmark}\]</span></p>
<p>No se pueden comparar los estimadores <span class="math inline">\(\hat{t}_{pwr}\)</span> y <span class="math inline">\(\hat{t}_{\pi}\)</span>. Ambos son insesgados, pero no se pude concluir sobre sus varianzas. La varianza del <span class="math inline">\(\hat{t}_{pwr}\)</span> depende de los valores de <span class="math inline">\(y_k\)</span>, lo cuales son desconocidos.</p>
</div>
<div id="el-estimador-hatt_alt" class="section level3">
<h3>El estimador <span class="math inline">\(\hat{t}_{alt}\)</span></h3>
<p><span class="math display">\[{\color{red} \star } \: \: \hat{t}_{alt} = N \, \bar{y}_S = \frac{N}{n_S} \sum\nolimits_s y_k\]</span></p>
<p>Este estimador tiene como inconveniente que el tamaño muestral <span class="math inline">\(n_S\)</span> es aleatorio. En general, <span class="math inline">\(\hat{t}_{alt}\)</span> tiene menor varianza que <span class="math inline">\(\hat{t}_{pwr}\)</span> o <span class="math inline">\(\hat{t}_{\pi}\)</span></p>
</div>
<div id="descomposición-de-la-varianza" class="section level3">
<h3>Descomposición de la varianza</h3>
<p>Supongamos que la población $ U = { 1; ; k; ; N }$ se encuentra particionada en <span class="math inline">\(a\)</span> grupos de tamaño <span class="math inline">\(n\)</span>, <span class="math inline">\(\{ S_1; \ldots; S_r, \ldots; S_a \}\)</span>. Luego entonces:
<span class="math display">\[\sum\nolimits_U \big( y_k - \bar{y}_U \big)^2 = \sum\limits_{r=1}^{a} \sum\nolimits_{S_r} \big( y_k - \bar{y}_U \big)^2 = \sum\limits_{r=1}^{a} \left[ \sum\nolimits_{S_r} \Big( ( y_k - \bar{y}_{S_r} ) + ( \bar{y}_{S_r} - \bar{y}_U ) \Big)^2 \right] =\]</span>
<span class="math display">\[= \sum\limits_{r=1}^{a} \Bigg[ \sum\nolimits_{S_r} \big( y_k - \bar{y}_{S_r} \big)^2 + 2 \sum\nolimits_{S_r} \big( y_k - \bar{y}_{S_r} \big) \big( \bar{y}_{S_r} - \bar{y}_U \big) + \sum\nolimits_{S_r} \big( \bar{y}_{S_r} - \bar{y}_U \big)^2 \Bigg] =\]</span>
<span class="math display">\[= \sum\limits_{r=1}^{a} \Bigg[ \sum\nolimits_{S_r} \big( y_k - \bar{y}_{S_r} \big)^2 + 2 \, \big( \bar{y}_{S_r} - \bar{y}_U \big) \underbrace{ \sum\nolimits_{S_r} \big( y_k - \bar{y}_{S_r} \big) }_{=0} + \sum\nolimits_{S_r} \big( \bar{y}_{S_r} - \bar{y}_U \big)^2 \Bigg] =\]</span>
<span class="math display">\[= \sum\limits_{r=1}^{a} \Bigg[ \sum\nolimits_{S_r} \big( y_k - \bar{y}_{S_r} \big)^2 + \sum\nolimits_{S_r} \big( \bar{y}_{S_r} - \bar{y}_U \big)^2 \Bigg] =\]</span>
<span class="math display">\[= \sum\limits_{r=1}^{a} \sum\nolimits_{S_r} \big( y_k - \bar{y}_{S_r} \big)^2 + \sum\limits_{r=1}^{a} \sum\nolimits_{S_r} \big( \bar{y}_{S_r} - \bar{y}_U \big)^2 =\]</span>
<span class="math display">\[= \sum\limits_{r=1}^{a} \sum\nolimits_{S_r} \big( y_k - \bar{y}_{S_r} \big)^2 + \sum\limits_{r=1}^{a} n \big( \bar{y}_{S_r} - \bar{y}_U \big)^2\]</span></p>
<p>Por lo tanto, tenemos que:
<span class="math display">\[\underbrace{ \sum\nolimits_U \big( y_k - \bar{y}_U \big)^2 }_{SST} = \underbrace{ \sum\limits_{r=1}^{a} \sum\nolimits_{S_r} \big( y_k - \bar{y}_{S_r} \big)^2 }_{SSW} + \underbrace{ \sum\limits_{r=1}^{a} n \big( \bar{y}_{S_r} - \bar{y}_U \big)^2 }_{SSB} \Rightarrow \color{blue}\boxed{ SST = SSW + SSB }\]</span></p>
</div>
<div id="tamaño-muestral" class="section level3">
<h3>Tamaño muestral</h3>
<p>Supongamos que se quiere estimar <span class="math inline">\(t_y\)</span> usando el estimador <span class="math inline">\(\hat{t}_{y}\)</span> y el estimador de su varianza <span class="math inline">\(\hat{V}_{p(s)}(\hat{t}_y)\)</span>. Supongamos que ambos estimadores son (aprox) insesgados y que es razonable suponer que:
<span class="math display">\[\frac{\hat{t}_y - t_y}{\sqrt{V_{p(s)}(\hat{t}_y)}} \overset{a}{\sim} \text{N}(0;1)\]</span></p>
<p>Buscamos un <span class="math inline">\(\E(n_S)\)</span> tal que para una precisión dada, <span class="math inline">\(\varepsilon &gt; 0\)</span>, y un nivel de confianza dado, <span class="math inline">\(0 &lt; \alpha &lt; 1\)</span>, nos permita plantear:
<span class="math display">\[\Pr \big(|\hat{t}_y - t_y| &lt; \varepsilon \big) \doteq 1 - \alpha \Rightarrow \Pr \left(\hat{t}_y - z_{1 - \, ^{\alpha}\!/_2} \sqrt{\hat{V}_{p(s)}(\hat{t}_y)} &lt; t_y &lt; \hat{t}_y + z_{1 - \, ^{\alpha}\!/_2} \sqrt{\hat{V}_{p(s)}(\hat{t}_y)} \right) \doteq 1 - \alpha\]</span>
<span class="math display">\[\Pr \big(|\hat{t}_y - t_y| &lt; \varepsilon \big) \doteq 1 - \alpha \Rightarrow \Pr \left( \frac{|\hat{t}_y - t_y|}{\sqrt{V_{p(s)}(\hat{t}_y)}} &lt; \frac{\varepsilon}{\sqrt{V_{p(s)}(\hat{t}_y)}} \right) \doteq 1 - \alpha\]</span></p>
<p>Si <span class="math inline">\(\frac{\varepsilon}{\sqrt{{\V}_{p(s)}(\hat{t}_y)}} \doteq z_{1 - \, ^{\alpha}\!/_2} \Rightarrow \varepsilon^2 \doteq z_{1 - \, ^{\alpha}\!/_2}^2 {\V}_{p(s)}(\hat{t}_y)\)</span>, donde <span class="math inline">\(\varepsilon\)</span> y <span class="math inline">\(\alpha\)</span> están fijos y, en general, <span class="math inline">\({\V}_{p(s)}(\hat{t}_y)\)</span> depende de <span class="math inline">\(n\)</span> y de <span class="math inline">\(S^2_{y_U}\)</span>. Luego si se cuenta con una buena estimación de <span class="math inline">\(S^2_{y_U}\)</span>, se puede despejar <span class="math inline">\(n\)</span>.</p>
<p>Para un tamaño de muestra fijo, si disminuimos <span class="math inline">\(\varepsilon\)</span>, reducimos la amplitud del intervalo, con lo que reducimos la confianza, o sea, aumentamos <span class="math inline">\(\alpha\)</span>.</p>
<p>En la práctica <span class="math inline">\(S^2_{y_U}\)</span> es desconocida, pero se pueden ensayar alguna de las siguientes estrategias para obtener una aproximación:</p>
<ul>
<li>Se presume algún tipo de distribución para los valores de <span class="math inline">\(y\)</span> en la población. Luego se busca una cota para <span class="math inline">\(S^2_{y_U}\)</span>.
<ul>
<li>Si <span class="math inline">\(y \sim \text{Ber} \Rightarrow 0 \leq S^2_{y_U} \leq \,^1\!/_4\)</span></li>
<li>Si <span class="math inline">\(y \overset{a}{\sim} \text{N} \Rightarrow S^2_{y_U} \doteq \frac{y_{k_{(n)}} - y_{k_{(1)}} }{6}\)</span> para un <span class="math inline">\(\alpha \doteq 0.01\)</span></li>
</ul></li>
<li>Utilizar datos de algún relevamiento reciente o de alguna variable auxiliar para la que se pueda asumir una variabilidad similar. En estos casos se suele utilizar el <span class="math inline">\(CV_{y_u}\)</span> dado que es más estable que la varianza. Luego se fija una precisión relativa y se determina el valor de <span class="math inline">\(n\)</span>.</li>
</ul>
<p><span class="math display">\[\Pr \big(|\hat{t}_y - t_y| &lt; t_y \, \varepsilon \big) \doteq 1 - \alpha \Rightarrow \frac{t_y \, \varepsilon}{\sqrt{{\V}_{p(s)}(\hat{t}_y)}} = z_{1 - \, ^{\alpha}\!/_2} \Rightarrow \varepsilon = z_{1 - \, ^{\alpha}\!/_2} \left[ \frac{{\V}_{p(s)}(\hat{t}_y)}{t_y} \right]\]</span></p>
<ul>
<li>Tomar una pequeña “muestra de iluminación” y calcular <span class="math inline">\(S^2_{y_s}\)</span> en dicha muestra. Luego se utilizar esta estimación como estimador de <span class="math inline">\(S^2_{y_U}\)</span> para calcular <span class="math inline">\(n\)</span>.</li>
</ul>
</div>
<div id="desarrollos-de-taylor-para-aproximaciones-en-muestreo-de-poblaciones-finitas" class="section level3">
<h3>Desarrollos de Taylor para aproximaciones en muestreo de poblaciones finitas</h3>
<p>Supongamos que queremos estimar: <span class="math inline">\(\theta = f\big( t_1; \ldots; t_q \big) = f \big( \mathbf{t} \big)\)</span> donde <span class="math inline">\(t_j = \sum\nolimits_U y_{jk}\)</span> <span class="math inline">\(j=1;\ldots;q\)</span>, son los totales de las <span class="math inline">\(q\)</span> variables poblacionales relevadas. Un estimador podría ser <span class="math inline">\(\hat{\theta} = f\big( \hat{t}_{1 \, \pi}; \ldots; \hat{t}_{q \, \pi} \big) = f \big( \hat{\mathbf{t}}_{\pi} \big)\)</span> donde <span class="math inline">\(\hat{t}_j = \sum\nolimits_s y_{jk}^{\checkmark}\)</span> <span class="math inline">\(j=1;\ldots;q\)</span>, son los totales de las <span class="math inline">\(q\)</span> variables poblacionales relevadas, estimados en la muestra <span class="math inline">\(s\)</span>.</p>
<p>Si <span class="math inline">\(f \big( \hat{\mathbf{t}}_{j \, \pi} \big)\)</span> es lineal tenemos que <span class="math inline">\(\theta = a_0 + \sum\limits_{j=1}^{q} a_j \, t_j = a_0 + \mathbf{a}&#39; \mathbf{t}\)</span>. Luego entonces <span class="math inline">\(\hat{\theta} = a_0 + \sum\limits_{j=1}^{q} a_j \, \hat{t}_{\pi \, j} = a_0 + \mathbf{a}&#39; \hat{\mathbf{t}}_{\pi}\)</span> estima <span class="math inline">\(\theta\)</span> de forma tal que:</p>
<ul>
<li><span class="math inline">\({\E} \big( \hat{\theta} \big) = {\E} \big( a_0 + \mathbf{a}&#39; \hat{\mathbf{t}}_{\pi} \big) = {\E}(a_0) + {\E}(\mathbf{a}&#39; \hat{\mathbf{t}}_{\pi}) = a_0 + \mathbf{a}&#39; \mathbf{t} \Rightarrow \hat{\theta}\)</span> es insesgado para <span class="math inline">\(\theta\)</span>.</li>
<li><span class="math inline">\({\V}(\hat{\theta}) = {\V} \big( a_0 + \mathbf{a}&#39; \hat{\mathbf{t}}_{\pi} \big) = \sum\limits_{j=1}^{q} \sum\limits_{j=1}^{q} a_{jj} \C(\hat{t}_{j \, \pi}; \hat{t}_{j&#39; \, \pi} ) = \mathbf{a}&#39; \, V(\hat{\mathbf{t}}_{\pi}) \, \mathbf{a}\)</span>, donde</li>
</ul>
<p><span class="math display">\[\C(\hat{t}_{j \, \pi}; \hat{t}_{j&#39; \, \pi} ) = \sum\sum\nolimits_{U} \Delta_{kl} \, y_{jk}^{\checkmark} \, y_{j&#39;k}^{\checkmark}\]</span></p>
<p>Podemos reescribir <span class="math inline">\(\hat{\theta}\)</span> de la siguiente forma:</p>
<p><span class="math display">\[{\color{red} \star} \: \: \hat{\theta} = a_0 + \sum\limits_{j=1}^{q} a_j \, \hat{t}_{j \, \pi} = a_0 + \mathbf{a}&#39; \hat{\mathbf{t}}_{\pi} = a_0 + \sum\limits_{j=1}^{q} a_j \sum\nolimits_s y_{jk}^{\checkmark} = a_0 + \sum\limits_{j=1}^{q} \sum\nolimits_s a_j \, y_{jk}^{\checkmark} = a_0 + \sum\nolimits_s u_k^{\checkmark}\]</span>
<span class="math display">\[\text{ con } u_k = \sum\limits_{j=1}^{q} a_j \, y_{jk} \text{ y } u_k^{\checkmark} = \frac{u_k}{\pi_k}\]</span></p>
<p><span class="math display">\[{\color{red} \star} \: \: {\V}(\hat{\theta}) = \sum\sum\nolimits_U \Delta_{kl} \, u_k^{\checkmark} \, u_l^{\checkmark}\]</span></p>
<p><span class="math display">\[{\color{red} \star} \: \: \hat{\V}(\hat{\theta}) = \sum\sum\nolimits_s \Delta_{kl}^{\checkmark} \, u_k^{\checkmark} \, u_l^{\checkmark}\]</span></p>
<p>Si <span class="math inline">\(f \big( \hat{\mathbf{t}}_{\pi} \big)\)</span> no es lineal, <span class="math inline">\(\hat{\theta}\)</span> debe aproximarse linealmente, y luego se podrán calcular <span class="math inline">\(\V(\hat{\theta})\)</span> y <span class="math inline">\(\hat{\V}(\hat{\theta})\)</span>. La técnica aproxima <span class="math inline">\(\hat{\theta}\)</span> por un pseudo-estimador, <span class="math inline">\(\hat{\theta}_0\)</span>, que es lineal en <span class="math inline">\(\hat{\mathbf{t}}_{\pi}\)</span>. En general <span class="math inline">\(\hat{\theta}_0\)</span> dependerá de cantidades desconocidas (de ahí que se le llama pseudo-estimador). La técnica para hallar <span class="math inline">\(\hat{\theta}_0\)</span> consiste en la aproximación de Taylor de primer orden de la función <span class="math inline">\(f\)</span>, en el entorno de un punto <span class="math inline">\(\mathbf{t}\)</span>, y despreciar el término de error.</p>
<p><span class="math display">\[\hat{\theta} \doteq \hat{\theta}_0 = \theta + \sum\limits_{j=1}^{q} a_j \big( \hat{t}_{j \, \pi} - t_j \big) \,\,\, \text{ donde } \,\,\, a_j = \frac{\partial f}{\partial \hat{t}_{j \, \pi}} \Bigg| _{ \hat{\mathbf{t}}_{\pi} = \mathbf{t} }\]</span></p>
<p>En muestras grandes <span class="math inline">\(\hat{\mathbf{t}}_{\pi} \approx \mathbf{t} \Rightarrow \hat{\theta}_0 = \hat{\theta}\)</span> y <span class="math inline">\(\AV(\hat{\theta}) = \V( \hat{\theta}_0 )\)</span>.</p>
<p><span class="math display">\[{\color{red} \star} \: \: \AV( \hat{\theta} ) \doteq V( \hat{\theta}_0 ) = V \left( \sum\limits_{j=1}^{q} a_j \, \hat{t}_{j \, \pi} \right) = V \left( \sum\limits_{j=1}^{q} a_j \sum\nolimits_s \frac{ y_{jk} }{ \pi_k } \right) =\]</span>
<span class="math display">\[= \V \left( \sum\nolimits_s u_k^{\checkmark} \right) = \sum\sum\nolimits_U \Delta_{kl} \, u_k^{\checkmark} \, u_l^{\checkmark}\]</span></p>
<p>Como <span class="math inline">\(\E(\hat{\theta}_0) = \theta \Rightarrow MSE (\hat{\theta}) \doteq MSE (\hat{\theta}_0) = \V (\hat{\theta}_0) = \AV (\hat{\theta})\)</span></p>
<p>Las cantidades <span class="math inline">\(u_k\)</span> dependen de <span class="math inline">\(a_j = \frac{\partial f}{\partial \hat{t}_{j \, \pi}} \Bigg| _{ \hat{\mathbf{t}}_{\pi} = \mathbf{t} }\)</span> que es desconocida ya que <span class="math inline">\(t_j\)</span> es desconocido <span class="math inline">\(\forall j\)</span>. De todas formas, la estimación puntual será <span class="math inline">\(\hat{\theta}_0 = f \big( \hat{\mathbf{t}}_{\pi} \big)\)</span>. Para estimar la varianza se reemplaza <span class="math inline">\(a_j\)</span> por <span class="math inline">\(\hat{a}_j = \frac{\partial f}{\partial \hat{t}_{j \, \pi}} \Bigg| _{ \hat{\mathbf{t}}_{\pi} = \hat{\mathbf{t}}_0 }\)</span>, siendo <span class="math inline">\(\hat{\mathbf{t}}_0\)</span> el total observado en la muestra. Luego entonces <span class="math inline">\(\hat{u}_k = \sum\limits_{j=1}^{q} \hat{a}_j \, y_{jk}\)</span>, con lo que puede calcularse <span class="math inline">\(\hat{\V} (\hat{\theta}) = \sum\sum\nolimits_s \Delta_{kl}^{\checkmark} \, \hat{u}_k^{\checkmark} \, \hat{u}_l^{\checkmark}\)</span>. Esto es válido ya que <span class="math inline">\(\hat{u}_k\)</span> es consistente para estimar <span class="math inline">\(u_k\)</span>.</p>
</div>
<div id="estimador-de-una-razón" class="section level3">
<h3>Estimador de una razón</h3>
<p>El problema consiste en estimar un cociente entre totales poblacionales:
<span class="math display">\[{\color{red} \star} \: \: R = \frac{t_y}{t_z} = \frac{\bar{y}_U}{\bar{z}_U}\]</span></p>
<p>Sea el estimador:
<span class="math display">\[{\color{red} \star} \: \: \hat{R} = f \big( \hat{t}_{y \, \pi}; \hat{t}_{z \, \pi} \big) = \frac{ \hat{t}_{y \, \pi} }{ \hat{t}_{z \, \pi} } = \frac{\bar{y}_s}{\bar{z}_s}\]</span></p>
<p>Utilizando la linealización de Taylor:
<span class="math display">\[\hat{R} = \hat{R}_0 = R + a_1 \big( \hat{t}_{y \, \pi} - t_y \big) + a_2 \big( \hat{t}_{z \, \pi} - t_z \big)\]</span>
<span class="math display">\[\text{donde } \,\,\, a_1 = \frac{\partial f}{\partial \hat{t}_{y \, \pi}} \Bigg|_{\substack{ \hat{t}_{y \, \pi} = t_y \\ \hat{t}_{z \, \pi} = t_z}} = \frac{1}{t_z} \,\,\, \text{ y } \,\,\, a_2 = \frac{\partial f}{\partial \hat{t}_{z \, \pi}} \Bigg|_{\substack{ \hat{t}_{y \, \pi} = t_y \\ \hat{t}_{z \, \pi} = t_z}} = - \frac{t_y}{t_z^2} = - \frac{R}{t_z}\]</span></p>
<p>Luego entonces:
<span class="math display">\[\hat{R} \doteq \hat{R}_0 = R + \frac{1}{t_z} \big( \hat{t}_{y \, \pi} - t_y \big) - \frac{R}{t_z} \big( \hat{t}_{z \, \pi} - t_z \big) = R + \frac{\hat{t}_{y \, \pi}}{t_z} - \frac{t_y}{t_z} - \frac{R \, \hat{t}_{z \, \pi}}{t_z} + \frac{R \, t_z}{t_z} =\]</span>
<span class="math display">\[= R + \frac{\hat{t}_{y \, \pi}}{t_z} - R - \frac{R \, \hat{t}_{z \, \pi}}{t_z} + R = R + \frac{\hat{t}_{y \, \pi}}{t_z} - \frac{R \, \hat{t}_{z \, \pi}}{t_z} = R + \frac{1}{t_z} \big( \hat{t}_{y \, \pi} - R \, \hat{t}_{z \, \pi} \big) =\]</span>
<span class="math display">\[= R + \frac{1}{t_z} \sum\nolimits_s \frac{y_k - R \, z_k}{\pi_k} = R + \sum\nolimits_s \frac{u_k}{\pi_k} \,\,\, \text{ donde } \,\,\, u_k = \frac{1}{t_z} \big( y_k - R \, z_k \big)\]</span></p>
<p>En conclusión:
<span class="math display">\[\color{blue}\boxed{ \hat{R} \doteq \hat{R}_0 = R + \sum\nolimits_s \frac{u_k}{\pi_k} }\]</span></p>
<p><span class="math inline">\(\hat{R} \doteq \hat{R}_0\)</span> es aproximadamente insesgado para <span class="math inline">\(R\)</span>.
<span class="math display">\[{\color{red} \star } \: \: \E( \hat{R} ) \doteq \E( \hat{R}_0 ) = E \left( R + \sum\nolimits_s \frac{u_k}{\pi_k} \right) = R + \sum\nolimits_U u_k = R + \sum\nolimits_U \frac{y_k}{t_z} - R \sum\nolimits_U \frac{z_k}{t_z} =\]</span>
<span class="math display">\[= R + \frac{1}{t_z} \sum\nolimits_U y_k - \frac{R}{t_z} \sum\nolimits_U z_k = R + \frac{t_y}{t_z} - \frac{R}{t_z} \, t_z = \frac{t_y}{t_z} = R\]</span>
<span class="math display">\[{\color{red} \star } \: \: \AV( \hat{R} ) = \V( \hat{R}_0 ) = \sum\sum\nolimits_U \Delta_{kl} \, u_k^{\checkmark}  \, u_l^{\checkmark} = \frac{1}{t_z^2} \sum\sum\nolimits_U \Delta_{kl} \, E_k^{\checkmark} \, E_l^{\checkmark}\]</span>
<span class="math display">\[\text{ donde } \: \: E_k = y_k - R \, z_k\]</span>
<span class="math display">\[{\color{red} \star } \: \: \hat{\V}( \hat{R}_0 ) = \sum\sum\nolimits_s \Delta_{kl}^{\checkmark} \, \hat{u}_k^{\checkmark}  \, \hat{u}_l^{\checkmark} = \frac{1}{\hat{t}_z^2} \sum\sum\nolimits_s \Delta_{kl}^{\checkmark} \, e_k^{\checkmark} \, e_l^{\checkmark}\]</span>
<span class="math display">\[\text{ donde } \: \: e_k = y_k - \hat{R} \, z_k\]</span>
<span class="math display">\[{\color{red} \star } \: \: \E \big( \hat{\V}( \hat{R}_0 ) \big) = \E \left( \sum\sum\nolimits_s \Delta_{kl}^{\checkmark} \, \hat{u}_k^{\checkmark} \, \hat{u}_l^{\checkmark} \right) = \sum\sum\nolimits_U \Delta_{kl} \, \hat{u}_k^{\checkmark} \, \hat{u}_l^{\checkmark}\]</span></p>
<p>Por lo tanto, <span class="math inline">\(\hat{\V}( \hat{R}_0 )\)</span> es aproximadamente insesgado para estimar <span class="math inline">\(\V(\hat{R})\)</span>.</p>
<p>Las expresiones anteriores para <span class="math inline">\(\AV( \hat{R} )\)</span> y <span class="math inline">\(\hat{\V}( \hat{R})\)</span> son equivalentes a:
<span class="math display">\[{\color{red} \star } \: \: \AV ( \hat{R} ) = \frac{1}{t_z^2} \Big[ \V(\hat{t}_{y \, \pi}) + R^2 \, V(\hat{t}_{z \, \pi}) - 2 \, R \, \C \big( \hat{t}_{y \, \pi}; \hat{t}_{z \, \pi} \big) \Big]\]</span>
<span class="math display">\[{\color{red} \star } \: \: \hat{\V} ( \hat{R} ) = \frac{1}{\hat{t}_z^2} \Big[ \hat{\V}(\hat{t}_{y \, \pi}) + \hat{R}^2 \, \hat{\V}(\hat{t}_{z \, \pi}) - 2 \, \hat{R} \, \hat{\C} \big( \hat{t}_{y \, \pi}; \hat{t}_{z \, \pi} \big) \Big]\]</span></p>
</div>
<div id="el-estimador-hatt_y_ra" class="section level3">
<h3>El estimador <span class="math inline">\(\hat{t}_{y_{ra}}\)</span></h3>
<p>El objetivo es estimar <span class="math inline">\(t_y\)</span>, y se cuenta con una variable auxiliar <span class="math inline">\(z\)</span> conocida <span class="math inline">\(\forall k \in U\)</span>. Sea el ``estimador de razón’’:
<span class="math display">\[{\color{red} \star } \: \: \hat{t}_{yra} = \frac{ \hat{t}_{y \, \pi} }{ \hat{t}_{z \, \pi} } \, t_z = \hat{R} \, t_z = \frac{t_z}{\hat{t}_{z \, \pi}} \, \hat{t}_{y \, \pi}\]</span>
<span class="math display">\[{\color{red} \star } \: \: \AV \big( \hat{t}_{yra} \big) = \AV \big( \hat{R} \, t_z \big) = t_z^2 \, \AV \big( \hat{R} \big) = \sum\sum\nolimits_U \Delta_{kl} \, \left( \frac{y_k - R \, z_k}{\pi_k} \right) \left( \frac{y_l - R \, z_l}{\pi_l} \right)\]</span>
<span class="math display">\[{\color{red} \star } \: \: \hat{\V} \big( \hat{t}_{yra} \big) = \sum\sum\nolimits_s \Delta_{kl}^{\checkmark} \, \left( \frac{y_k - \hat{R} \, z_k}{\pi_k} \right) \left( \frac{y_l - \hat{R} \, z_l}{\pi_l} \right)\]</span></p>
<p>La lógica detrás de este estimador es la mismas que en el <span class="math inline">\(\hat{t}_{alt} = \frac{N}{\hat{N}} \, \hat{t}_{y \, \pi}\)</span> donde <span class="math inline">\(z_k = 1 \: \: \forall k \in U\)</span>.</p>
<hr />
<p>Si te sirvieron estas notas, podes encontrar la versión completa de las notas de muestreo 1 <a href="https://github.com/daczarne/udelar_muestreo_1/tree/master/Resumenes">aquí</a>. Las notas de muestreo 2 se encuentran <a href="https://github.com/daczarne/udelar_muestreo_2/tree/master/Resumenes">aquí</a>.</p>
</div>
</div>
