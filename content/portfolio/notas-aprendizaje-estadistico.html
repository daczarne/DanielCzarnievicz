---
date: '2020-04-13'
draft: false
title: "Notas sobre aprendizaje estadístico"
weight: 0
output: blogdown::html_page
---



<p>Resúmenes del curso de Análisis Multivariado I del Instituto de Estadística (IESTA - FCEA - Udelar).</p>
<!--more-->
<p>Aquí podrás encontrar algunos resúmenes sobre las técnicas de aprendizaje estadístico cubiertas en el curso Análisis Multivariado I del IESTA.</p>
<div id="análisis-de-clusters" class="section level2">
<h2>Análisis de Clusters</h2>
<div id="descripción-general" class="section level3">
<h3>Descripción general</h3>
<p>El objetivo general del análisis de clusters es formar grupos (<strong>clusters</strong>) de acuerdo a las características de interés. El procedimiento general puede describirse de la siguiente manera:</p>
<ul>
<li>Se parte de la matriz de datos <span class="math inline">\(\boldsymbol{X}_{I \times J}\)</span>, generalmente estandarizada.<br />
</li>
<li>Se define alguna forma de medir similitudes entre los individuos (medida de distancia).<br />
</li>
<li>Se crea la matriz <span class="math inline">\(\boldsymbol{D}_{I \times I}\)</span> donde el elemento <span class="math inline">\(d_{ij}\)</span> mide el grado de similitud entre los individous <span class="math inline">\(i\)</span> y <span class="math inline">\(j\)</span>.<br />
</li>
<li>Se definen algoritmos de clasificación.<br />
</li>
<li>Se definen stopping rules.<br />
</li>
<li>Se selecciona el número de grupos en función de las características consideradas.</li>
</ul>
<p>El análisis de clusters forma parte de los llamados métodos de clasificación no supervisada.</p>
<p>La metedología también puede aplicarse por variables, en lugar de por observaciones. El objetivo en estos casos es detectar similitudes o jerarquías entre las variables para luego utilizar métodos de reducción de dimensionalidad.</p>
</div>
<div id="medidas-de-distancia" class="section level3">
<h3>Medidas de distancia</h3>
<p>La similitud entre objetos es una medida de correspondencia, asociación o parecido entre objetos que van a ser agrupados. La semejanza puede ser definida mediante una función <span class="math inline">\(s_{ij}: \mathbb{R}^J \rightarrow \mathbb{R}\)</span>, donde <span class="math inline">\(s_{ij} = s_{ji} \,\,\, \forall i \forall j\)</span>. Para que <span class="math inline">\(s_{ij}\)</span> sea un índice de similitud debe cumplirse que <span class="math inline">\(s_{ij} \leq s_{ii} = s_{jj} \,\,\, \forall i \forall j\)</span>, y, en general, <span class="math inline">\(0 \leq s_{ij} \leq 1\)</span>.</p>
<p>Si <span class="math inline">\(s_{ij}\)</span> es un índice de similaridad, entonces <span class="math inline">\(d_{ij} = 1 - s_{ij}\)</span> es un índice de disimilaridad con:</p>
<ul>
<li><span class="math inline">\(0 \leq d_{ij} \leq 1\)</span><br />
</li>
<li><span class="math inline">\(d_{ij} = 0 \Leftrightarrow i = j\)</span>, por lo tanto, <span class="math inline">\(d_{ii} = d_{jj} = 0 \,\,\, \forall i \forall j\)</span><br />
</li>
<li><span class="math inline">\(d_{ij} = d_{ji}\)</span> (propiedad de simetría).</li>
</ul>
<p>Llamaremos <strong>distancia (en el espacio métrico E)</strong> a las disimilariadades que satisfacen:</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(d_{ij} &gt; 0 \,\,\, \forall (i,j) \in E \,\,\, \text{con} \,\,\, i \neq j\)</span><br />
</li>
<li><span class="math inline">\(d_{ij} = 0 \Leftrightarrow i = j \forall (i,j) \in E\)</span><br />
</li>
<li><span class="math inline">\(d_{ij} \leq d_{ik} + d_{kj} \forall (i,k,j) \in E\)</span> (desigualdad triangular)</li>
</ol>
<div id="algunas-medidas-de-distancia-comúnmente-utilizadas" class="section level4">
<h4>Algunas medidas de distancia comúnmente utilizadas</h4>
<p>Para variables cuantitativas y siendo <span class="math inline">\(p\)</span> el número de variables:</p>
<p><span class="math display">\[ \boxed{ \text{Dist. Euclídea: } d_{ij}^2 = \sum\limits_{k =1 }^{p} \big( x_{ik} - x_{jk} \big)^2 } \]</span></p>
<p><span class="math display">\[ \boxed{ \text{Dist. Euclídea Reducida: } d_{ij}^2 = \sum\limits_{k =1 }^{p} \frac{ \big( x_{ik} - x_{jk} \big)^2 }{s_k^2} } \]</span></p>
<p><span class="math display">\[ \boxed{ \text{Dist. Minkowski de orden t: } d_{ij}^2 = \sum\limits_{k =1 }^{p} \frac{ \big| x_{ik} - x_{jk} \big|^t }{s_k^{t} } } \]</span></p>
<p><span class="math display">\[ \boxed{ \text{Dist. Mahalanobis: } d_{ij}^2 = \big( x_{ik} - x_{jk} \big)&#39; \boldsymbol{\Sigma}^{-1} \big( x_{ik} - x_{jk} \big) } \]</span></p>
</div>
<div id="variables-cualitativas" class="section level4">
<h4>Variables cualitativas</h4>
<p>Cuando en los datos se consideran varias variables cualitativas y cuantitativas, las distancias antes descriptas no son buenas medidas de la disimilaridad entre los elementos muestrales. Para poder trabajar con ellas es construye el índice de similaridad global, a partir de los índices por variables. La similariadad entre los elementos <span class="math inline">\(i\)</span> y <span class="math inline">\(h\)</span> en las <span class="math inline">\(j = 1, \, \ldots, \, p\)</span> variables se define como:
<span class="math display">\[ s_{ih} = \frac{ \sum\limits_{j = 1}^{p} w_{jih} \, s_{jih} }{ \sum\limits_{j = 1}^{p} w_{jih} } \]</span>
donde <span class="math inline">\(s_{jih}\)</span> es el índice de similaridad entre los elementos <span class="math inline">\(i\)</span> y <span class="math inline">\(h\)</span> en la variable <span class="math inline">\(j\)</span>, y <span class="math inline">\(w_{jih}\)</span> es el peso asignado a la variable <span class="math inline">\(j\)</span>, pudiendo este ser incluso 1 o 0.</p>
<p>Los índices de similariadad para cada tipo de variable se construyen de la siguiente forma:</p>
<p><strong>Variables cualitativas</strong>: puede construirse por bloque o para cada variable.</p>
<ol style="list-style-type: decimal">
<li><p>Cuando se realiza por variable la similitud será 1 en los casos en que ambas unidades posean o no el atributo, y 0 en caso de que una de ellas lo posea y la otra no.</p></li>
<li><p>Cuando se las trata de forma conjunta se construyen tablas de asociación contando los atributos presentes en:</p>
<ul>
<li>ambos elementos: <span class="math inline">\(a\)</span></li>
<li>en <span class="math inline">\(i\)</span> y no en <span class="math inline">\(h\)</span>: <span class="math inline">\(b\)</span></li>
<li>en <span class="math inline">\(h\)</span> y no en <span class="math inline">\(i\)</span>: <span class="math inline">\(c\)</span></li>
<li>en ningún elemento: <span class="math inline">\(d\)</span></li>
</ul></li>
</ol>
<p>Luego la similitud puede construirse mediante:</p>
<ul>
<li><em>Prop. de coincidencias</em>: se calcula como el número total de coincidencias sobre el número total de atributos.
<span class="math display">\[s_{ij} = \frac{ a + d }{ n_a}\]</span></li>
<li><em>Prop. de apariciones</em>: proporción de veces donde el atributo aparece en ambas observaciones.
<span class="math display">\[s_{ij} = \frac{ a }{ a + b + c }\]</span></li>
</ul>
<p><strong>Variables cuantiativas</strong>:</p>
<p><span class="math display">\[s_{jih} = 1 - \frac{|x_{ij} - x_{hj}|}{ rg(x_j) }\]</span></p>
<p>Una vez construidas las similaridades la distancia puede defirnirse como <span class="math inline">\(d_{ij} = 1 - s_{ij}\)</span>, pero esta puede no cumplir la propiedad triangular. Para los casos en que la matriz sea semi definida positiva, dicha propiedad sí se cumplirá si se calcula la distancia como:
<span class="math display">\[d_{ij} = \sqrt{2(1 - s_{ij})}\]</span></p>
<div style="page-break-after: always;"></div>
</div>
</div>
<div id="grupos" class="section level3">
<h3>Grupos</h3>
<p>Existen distintos métodos a través de los cuales constriur los grupos. Los mismos se clasifican según sean <em>divisivos</em> o <em>agregativos</em>, y según sean <em>jerarquicos</em> o <em>no jerarquicos</em>.</p>
<ul>
<li><strong>Métodos divisivos</strong>: se parte de un solo grupo con <span class="math inline">\(I\)</span> individuos y se particiona hasta obetener <span class="math inline">\(I\)</span> grupos con 1 individuo cada uno.<br />
</li>
<li><strong>Métodos agregativos</strong>: se parte de <span class="math inline">\(I\)</span> grupos con 1 individuo cada uno y se agregan hasta obtener 1 grupo con <span class="math inline">\(I\)</span> individuos.<br />
</li>
<li><strong>Métodos jerárquicos</strong>: genera particiones solapadas. No permite la reasignación de unidades. Estrictamente, no genera grupos, sino la estructura de asociación en cadena que pueda existir entre los elementos. Dicha jerarquía puede utilizarse para conformar grupos.<br />
</li>
<li><strong>Métodos no jerárquicos</strong>: se predetermina la cantidad de grupos no solapados. Permite la reasignación de unidades.</li>
</ul>
<div id="métodos-jerárquico-agregativos" class="section level4">
<h4>Métodos jerárquico-agregativos</h4>
<p>En los métodos jerárquico-agregativos se parte de <span class="math inline">\(I\)</span> grupos con un individuo cada uno, los cuales se agregan hasta obtener 1 grupo con <span class="math inline">\(I\)</span> individuos. En cada paso, los individuos o grupos más parecidos se unen para formar un nuevo grupo. Esto implica que los individous agrupados en pasos anteriores no pueden cambiar de grupo (a esto nos referimos cuando hablamos de particiones no solapadas).</p>
<p>El primer paso es siempre igual y muy sencillo. Se parte de la matriz de datos <span class="math inline">\(\boldsymbol{X}_{I \times J}\)</span>, y se construye la matriz de distancia <span class="math inline">\(\boldsymbol{D}_{I \times I}\)</span> según la métrica seleccionada. Luego, las dos unidades más parecidas (es decir, la de menor distancia), se unen para formar un grupo.</p>
<p>El problema comienza con los pasos subsiguiente cuando deben unirse observaciones con grupos previamente formados, o grupos con grupos. La pregunta a la que se debe dar respuesta es ¿cuál es la distancia entre la observación <span class="math inline">\(k\)</span>, y el grupo formado por las observaciones <span class="math inline">\(i\)</span> y <span class="math inline">\(j\)</span>? (Análogamente uno podría preguntarse cuál es la distancia entre el grupo <span class="math inline">\(U_1\)</span> y el grupo <span class="math inline">\(U_2\)</span>, siendo estos dos grupos formados en pasos anteriores del algoritmo). Para definir estas distancias existen distintos criterios:</p>
<ul>
<li><strong>Single Linkage</strong>: <span class="math inline">\(d_{(i,j), k} = \min\{ d_{i,k}; \, d_{j,k} \}\)</span></li>
<li><strong>Complete Linkage</strong>: <span class="math inline">\(d_{(i,j), k} = \max\{ d_{i,k}; \, d_{j,k} \}\)</span></li>
<li><strong>Average Linkage</strong>: la distancia entre dos grupos es el prmedio entras las distancias entre pares de observaciones. Es sesgado hacia la formación de grupos con igual varianza.</li>
<li><strong>Centroide</strong>: define la distancia entre los grupos <span class="math inline">\(K\)</span> y <span class="math inline">\(L\)</span> como <span class="math inline">\(d_{(K, L)} = || \bar{x}_K - \bar{x}_L ||^2\)</span>. Tiene el beneficio de ser robusto a la presencia de outliers.</li>
<li><strong>Ward</strong>: la unión entre grupos se relaiza de forma tal de minimizar la varianza interna.
<span class="math display">\[\underbrace{\sum\limits_{i = 1}^{I} \sum\limits_{j = 1}^{J} (x_{ij} - \bar{x}_j)^2}_{ \text{ T (Total variance)}} = \underbrace{ \sum\limits_{k = 1}^{K} \sum\limits_{i = 1}^{n_k} \sum\limits_{j = 1}^{J} \big( x_{ij}^{(k)} - \bar{x}_j^{(k)} \big)^2 }_{\text{W (Within group variance)} } + \underbrace{ \sum\limits_{k = 1}^{K} \sum\limits_{j = 1}^{J} n_k \big( \bar{x}_j^{(k)} - \bar{x}_j \big)^2 }_{ \text{B (Between group variance)}} \]</span></li>
</ul>
<p>Esto puede verse en términos de distancias realizando las sumas en <span class="math inline">\(J\)</span>:</p>
<ul>
<li><span class="math inline">\(T\)</span> es la suma de las distancias entre cada una de las <span class="math inline">\(i\)</span> observaciones y el centroide de las observaciones, matemáticamente:</li>
</ul>
<p><span class="math display">\[\sum\limits_{i = 1}^{I} \sum\limits_{j = 1}^{J} (x_{ij} - \bar{x}_j)^2 = \sum\limits_{i = 1}^{I} d^2_{(i, G)}\]</span></p>
<ul>
<li><span class="math inline">\(W\)</span> es la suma de las distancias entre cada una de las observaciones y el centroide del grupo <span class="math inline">\(k\)</span>, matemáticamente:</li>
</ul>
<p><span class="math display">\[\sum\limits_{k = 1}^{K} \sum\limits_{i = 1}^{n_k} \sum\limits_{j = 1}^{J} \big( x_{ij}^{(k)} - \bar{x}_j^{(k)} \big)^2 = \sum\limits_{k = 1}^{K} \sum\limits_{i = 1}^{n_k} d^2_{(i, G_k)}\]</span></p>
<ul>
<li><span class="math inline">\(B\)</span> es la suma de <span class="math inline">\(n_k\)</span>-veces las distancias entre el centroide del grupo <span class="math inline">\(k\)</span> y el centroide total de la observaciones, matemáticamente:</li>
</ul>
<p><span class="math display">\[\sum\limits_{k = 1}^{K} \sum\limits_{j = 1}^{J} n_k \big( \bar{x}_j^{(k)} - \bar{x}_j \big)^2 = \sum\limits_{k = 1}^{K} n_k d^2_{(G_k, G)}\]</span></p>
<p>El crecimiento de la inercia intraclase puede medirse mediante:
<span class="math display">\[\Delta_{ (K \cup L) } = \frac{n_K \, n_L}{n_K + n_L} \, d^2_{(G_K, G_L)}\]</span></p>
<p>El algoritmo de Ward es sesgado hacia la formación de grupos de igual tamaño.</p>
<div id="características-generales" class="section level5">
<h5>Características generales</h5>
<ul>
<li>Ward y Centroide tienden a formar grupos más esféricos. Son menos sensibles a la presencia de outliers.<br />
</li>
<li>Complete Linkage tiende a producir grupos esféricos de diámetro muy parecidos.<br />
</li>
<li>Single Linkage es más sensible a la presencia de outliers (los cuales generan un efecto cadena). El algoritmo tiende a separar a los outliers dejando grupos unitarios para el final.</li>
</ul>
</div>
</div>
<div id="métodos-jerárquico-divisivos" class="section level4">
<h4>Métodos jerárquico-divisivos</h4>
<p>Trabaja en dirección opuesta a los métodos agregativos. Es decir, parte de 1 grupo con <span class="math inline">\(I\)</span> observaciones y en cada paso divide el grupo jerárquico del que parten buscando construir los grupos más disímiles.</p>
</div>
<div id="stopping-rules" class="section level4">
<h4>Stopping Rules</h4>
<p>Un problema en el análisis de clusters mediante métodos jerárquicos consiste en poder determinar cuántos grupos deben formarse. Es decir, cuándo debe frensarse el algortimo de unión y proceder a describirse los grupos hasta entonces formados. Para ayudar con esto se recuerre a los siguiente indicadores:</p>
<p><span class="math inline">\({\color{red} \star} \: \: \text{Inspección visual del dendrograma}\)</span></p>
<p><span class="math inline">\({\color{red} \star} \: \: \boldsymbol{R^2}\)</span>: el índice se calcula a cada paso teniendo en cuenta la relación entre la varianza interna (<span class="math inline">\(W\)</span>) y la varianza total (<span class="math inline">\(T\)</span>): <span class="math inline">\(R^2 = 1 - \frac{ W }{ T }\)</span>. Cuando se tienen <span class="math inline">\(I\)</span> grupos de 1 individuo, <span class="math inline">\(R^2 = 1\)</span>. Cuando se tiene un grupo de <span class="math inline">\(I\)</span> individuos, <span class="math inline">\(R^2 = 0\)</span>. Si al pasar de <span class="math inline">\(k + 1\)</span> grupos a <span class="math inline">\(k\)</span> grupos el aporte al <span class="math inline">\(R^2\)</span> no es significativo, nos quedamos con <span class="math inline">\(k+1\)</span> grupos.</p>
<p><span class="math inline">\({\color{red} \star} \: \: \boldsymbol{pseudo-F}\)</span>:
<span class="math display">\[F_{p(k-1); \, p(n-k)} = \frac{ \text{tr}(B) / (k-1) }{ \text{tr}(W) / (n-k) } = \frac{ R^2 / (k-1) }{ (1 - R^2) / (n-k) }\]</span></p>
<ul>
<li>Si <span class="math inline">\(F\)</span> crece de forma monótona al crecer la cantidad de grupos <span class="math inline">\(k\)</span>, entonces no se puede determinar una clara estructura de grupos.</li>
<li>Si <span class="math inline">\(F\)</span> decrece de forma monótona al crecer la cantidad de grupos <span class="math inline">\(k\)</span>, entonces no se puede determinar una clara estructura de grupos, pero se puede decir que existe una estructura jerárquica.</li>
<li>Si <span class="math inline">\(F\)</span> presenta un máximo en <span class="math inline">\(k\)</span> grupos, entonces la población presenta un número definido de grupos en dicho máximo.</li>
</ul>
<p><span class="math inline">\({\color{red} \star} \: \: \boldsymbol{pseudo-t^2}\)</span>:
<span class="math display">\[t^2 = \frac{ \text{tr}(W_{G,L}) - \big[ \text{tr}(W_G) + \text{tr}(W_L) \big] }{ \big[ \text{tr}(W_G) + \text{tr}(W_L) \big] / (n_G + n_L - 2) }s\]</span>
Si al pasar de <span class="math inline">\(k+1\)</span> a <span class="math inline">\(k\)</span> grupos el índice presenta una caída muy grande, entonces nos quedamos con <span class="math inline">\(k+1\)</span> grupos. La idea es que el aumento de la heterogeneidad es demasiado grande y no conviene unir los grupos.</p>
</div>
<div id="métodos-no-jerárquico" class="section level4">
<h4>Métodos no jerárquico</h4>
<p>Los métodos no jerárquicos son también conocidos como métodos de partición, dado que se basan en particionar los datos en una cantidad predeterminada de grupos, <span class="math inline">\(G\)</span>. Una forma de trabajar este problema sería construir todos los posibles <span class="math inline">\(G\)</span> grupos en los que las <span class="math inline">\(n\)</span> observaciones podrían particionarse. El problema de esto es que la cantidad posible de grupos a estudiar es prohibitivamente grande incluso para valores moderados de <span class="math inline">\(n\)</span>.</p>
<div id="k-medias" class="section level5">
<h5>k-medias</h5>
<p>El algoritmo más utilizado para realizar dichas particiones es conocido como <strong>k-medias</strong>, y requiere de las siguientes cuatro etapas:</p>
<ul>
<li>Seleccionar <span class="math inline">\(G\)</span> puntos como centros iniciales de los grupos.<br />
</li>
<li>Calcular la distancia euclidea de cada observación a los centros de cada grupo, y asignar cada una de ellas al centro más próximo. Cada vez que una observación es asignada a un grupo, se re calcula el centro de dicho grupo.<br />
</li>
<li>Definir un criterio de optimalidad y comprobar si reasignar uno a uno cada elemento de un grupo a otro mejora el criterio.<br />
</li>
<li>Si no es posible mejorar el criterio, terminar el proceso.</li>
</ul>
<p>El método es sensible a la elección de los centros iniciales. Siempre se recomienda realizar el procedimiento con varios sets de centros iniciales. Si los resultados cambian drásticamente entre un set y otro, o si el algoritmo demora mucho tiempo en converger, puede deberse a que no exista una estructura de grupos en los datos.</p>
<p>k-medias también puede utilizarse en conjunto con métodos jerárquicos de la siguiente forma. Luego de construidos los clusters mediante algún método jerárquico, los centros de dicho grupos se utilizan como puntos iniciales para el algoritmo jerárquico.</p>
<p>Un criterio de optimalidad u homogeneidad comúnmente utilizado es minimizar la <em>suma de cuadrados dentro de los grupos</em> (SCDG):
<span class="math display">\[\min\{ SCDG \} = \min\left\{ \sum\limits_{g = 1}^{G} \sum\limits_{j = 1}^{p} \sum\limits_{i = n_g}^{G} ( x_{ijg} - \bar{x}_{jg} )^2 \right\} = \min\left\{ \sum\limits_{g = 1}^{G} \sum\limits_{j = 1}^{p} n_g \, s_{jg}^2 \right\}\]</span>
donde <span class="math inline">\(x_{ijg}\)</span> es el valor de la variable <span class="math inline">\(j\)</span> en el elemento <span class="math inline">\(i\)</span> del grupo <span class="math inline">\(g\)</span>, <span class="math inline">\(\bar{x}_{jg}\)</span> es la media de dicha variable en dicho grupo, <span class="math inline">\(n_g\)</span> es la cantidad de observaciones en el grupo <span class="math inline">\(g\)</span>, y <span class="math inline">\(s_{jg}^2\)</span> es la varianza muestral de la variable <span class="math inline">\(j\)</span> en el grupo <span class="math inline">\(g\)</span>.</p>
<p>Dado que encontrar la optimalidad de este criterio implicaría calcularlo para todas las posibles particiones de las <span class="math inline">\(n\)</span> observaciones, se agrega la restricción de que en cada iteración del algoritmo, solo una observación sea reasignada. El algoritmo se implementa de la siguiente forma entonces:</p>
<ul>
<li>Partir de una asignación inicial.</li>
<li>Comprobar si moviendo algún elemento se reduce <span class="math inline">\(\boldsymbol{W}\)</span>.</li>
<li>Si es posible, mover el elemento, recalcular las medias de los dos grupos afectados por el cambio, y volver al paso anterior. Si no es posible reducir <span class="math inline">\(\boldsymbol{W}\)</span>, terminar.</li>
</ul>
<p>Este mismo algoritmo puede utilizarse tomando otro tipos de centros. Por ejemplo, k-medoides toma como centros iniciales a los individuos representativos de cada grupo. Por representativos nos referimos a los individos que minimizan la disimiliaridad promedio a todos los objetos del cluster. De este forma, se logra que los centros efectivamente sean individuos pertenecientes a la muestra y no puntos ficticios. El método es menos sensible a la presencia de outliers.</p>
<p>Para definir los medoides, supongamos que <span class="math inline">\(x_1, \, \ldots, \, x_n\)</span> son puntos en un espacio métrico <span class="math inline">\((X, \, d)\)</span>. Entonces, el medoide se define como:
<span class="math display">\[x_{medoide} = \min\limits_{y \in \{ x_1, \, \ldots, \, x_n \}} \sum\limits_{i = 1}^{n} d(y, \, x_i)\]</span></p>
</div>
<div id="knn" class="section level5">
<h5>KNN</h5>
<p>KNN busca utilizar el clasificador de Bayes pero, en la práctica, no es posible conocer la distribución de <span class="math inline">\(Y|X\)</span>. Por lo tanto, el clasificador de Bayes no puede ser calculado. KNN estima la distrubición de <span class="math inline">\(Y|X\)</span>, y asigna las observaciones a la clase con mayor probabilidad estimada.</p>
<p>Dado un <span class="math inline">\(K \in \mathbb{N}\)</span> y una observación <span class="math inline">\(x_0\)</span>, KNN primero identifica los <span class="math inline">\(k\)</span> vecinos más cercanos. Estos constituyen el conjunto <span class="math inline">\(\mathcal{N}_0\)</span>. Luego se estima la probabilidad condicional para la clase <span class="math inline">\(j\)</span> como la fracción de puntos en <span class="math inline">\(\mathcal{N}_0\)</span> cuya variable de respuesta es igual a <span class="math inline">\(j\)</span>, esto es:
<span class="math display">\[\Pr(Y = j | X = x_0) = \frac{1}{K} \sum\limits_{i \in \mathcal{N}_0} \text{I}_{(y_i = j)}\]</span></p>
<p>Cuando <span class="math inline">\(K = 1\)</span>, KNN es demasiado flexible y encuntra patrónes en los datos que no se corresponden con el clasificador de Bayes. El error de training es cero, pero el error en el grupo de test puede ser muy alto. A medida que <span class="math inline">\(K\)</span> aumenta, el método se vuelve menos flexible y, eventualmente, produce un clasificador que es lineal.</p>
</div>
</div>
<div id="clusters-basados-en-modelos" class="section level4">
<h4>Clusters basados en modelos</h4>
<p>La clusterización basada en modelos es un método en el cual se asume la existencia de un modelo matemático, y se busca optimizar el ajuste entre el modelo y los datos. Generalmente, dicho modelo es una mezcla de distribuciones. Cada distribución determina la probabilidad de que una observación tenga un conjunto particular de atributo-valores, dado que pertenece a una de las <span class="math inline">\(k\)</span> distribuciones.</p>
<div id="algorítmo-em-expectation-maximazation" class="section level5">
<h5>Algorítmo EM (Expectation Maximazation)</h5>
<p>El algoritmo EM busca la estimación de los parámetros de las distribuciones que mejor se adaptan a los datos y al modelo propuesto (máxima verosimilitud). Con ello se estima el grado de pertenencia de cada observación a cada grupo.</p>
<p>El algoritmo comienza con la estimación inicial de los parámetros de las distribuciones (maximization) y los utiliza para calcular las probabilidad de que cada observación pertenezca a un cluster (expectation). Luego utiliza dichas probabilidades para re-estimar los parámetros de las distribuciones. El procedimiento se repite hasta converger. El ajuste global del modelo se evalúa a través del BIC.</p>
</div>
</div>
<div id="fuzzy-sets" class="section level4">
<h4>Fuzzy sets</h4>
<p>La clusterización fuzzy es una generalización de los métodos de partición. En estos, cada observación es asignada a un y solo un grupo. Los algoritmos fuzzy se basan en computar un <em>coeficiente de membresía</em> para cada observación y cada cluster. El coeficiente toma valores entre 0 y 1, de forma tal que la suma de todos los coeficientes para una misma observación es igual a 1.</p>
<p>El algoritmo fuzzy no utiliza individuos representativos, sino que busca minimizar la función objetivo:
<span class="math display">\[C = \sum\limits_{v = 1}^{k} \frac{ \sum\limits_{i, j = 1}^{n} u_{iv}^2 \, u_{jv}^2 \, d(i,j) }{ 2 \, \sum\limits_{j = 1}^{n} u_{ju}^2 }\]</span>
donde <span class="math inline">\(u_{iv}\)</span> es el coeficiente de membresía de la obserción <span class="math inline">\(i\)</span> en el cluster <span class="math inline">\(v\)</span>. La expresión del denominador aparece multiplicada por dos dado que se están sumando tanto el par <span class="math inline">\((i,j)\)</span> como el <span class="math inline">\((j,i)\)</span>. A su vez, dado que estamos sumando sobre todos los clusters posibles, la función es entonces una medida de dispersión total entre dichos clusters. El algoritmo itera hasta que el cambio en la función objetivo es menor a una tolerancia <span class="math inline">\(\varepsilon\)</span>.</p>
<p>Algunas clusterizaciones son más difuzas que otras. Cuando un individuo tiene igual coeficiente de membresía en todos los clusters (todos iguales a <span class="math inline">\(1/k\)</span>), decimos que presenta <em>complete fuzziness</em>. Por otro lado, cuando un individuo tiene coeficiente 1 en un cluster y 0 en todos los demás, estamos frente a un caso de partición. Para medir que tan rígida es la clusterización se utiliza el coeficiente de partición de Dunn:
<span class="math display">\[F_k = \sum\limits_{i = 1}^{n} \sum\limits_{v = 1}^{k} \frac{ u_{iv}^2 }{ n }\]</span>
Cuando la clusterización es completamente disfuza, <span class="math inline">\(F_k = 1/k\)</span>, mientras que cuando la clusterización es completamente particionada, toma valor 1. La versión normalizada del coeficiente de Dunn viene dada por la fórmula:
<span class="math display">\[F&#39;_k = \frac{ F_k - (1/k) }{ 1 - (1/k) } = \frac{ k \, F_k - 1 }{ 1 - k }\]</span>
la cual siempre toma valores entre 1 y 0, independientemente de la cantidad de clusters <span class="math inline">\(k\)</span> elegidos.</p>
<p>Habitualmente, las observaciones son asignadas al cluster para el cual tienen mayor coeficiente de membresía.</p>
</div>
</div>
<div id="silhouette" class="section level3">
<h3>Silhouette</h3>
<p>Un gráfico de Silhouette es una forma de validar el resultado de una clusterización. En el mismo se mide la cohesión (que tan similar es cada observación a los miembros de su propio cluster), y la separación (que tan disimil es cada observación a los miembros de los demás clusters). El Silhouette toma valores entre -1 y 1, donde valores altos indican que la observación está bien correspondida con los elementos de su propio cluster, y mal matcheada con los elementos de los demás clusters. Si la mayoría de las observaciones tienen valores altos de Silhouette, entonces la clusterización es considerada buena. Si, en cambio, varias observaciones tienen valores muy bajos, entonces es evidencia de que se consideraron ya sea muchos o muy pocos clusters.</p>
<p>La Silhouette se calcula con una métrica de distancia. Así, dada una clusterización de <span class="math inline">\(k\)</span> clusters, definimos <span class="math inline">\(a(i)\)</span> como la distancia media entre la observación <span class="math inline">\(i \in \mathcal{C}_i\)</span>, es decir:
<span class="math display">\[a(i) = \frac{1}{|\mathcal{C}_i| - 1} \sum\limits_{j \in \mathcal{C}_i, \, i \neq j} d(i,j)\]</span>
<span class="math inline">\(a(i)\)</span> puede interpretarse entonces como una medida de qué tan bien asignada está la observación <span class="math inline">\(i\)</span> en el cluster <span class="math inline">\(\mathcal{C}_i\)</span>.</p>
<p>De forma similar, definimos <span class="math inline">\(b(i)\)</span> como la menor disimilaridad promedio entre la observación <span class="math inline">\(i\)</span>, y los miembros de los demás clusters <span class="math inline">\(\mathcal{C}\)</span> con <span class="math inline">\(\mathcal{C} \neq \mathcal{C}_i\)</span>.
<span class="math display">\[b(i) = \min\limits_{k \neq i} \left\{ \frac{1}{|\mathcal{C}_k|} \sum\limits_{j \in \mathcal{C}_k} d(i,j) \right\}\]</span></p>
<p>Definimos la medida de silhouette para la observación <span class="math inline">\(i\)</span>, como:
<span class="math display">\[s(i) = \left\{
\begin{array}{ccc}
\frac{ b(i) - a(i) }{ \max \left\{ a(i), \, b(i) \right\} } &amp; \text{si} &amp; |\mathcal{C}_i| &gt; 1 \\ \\
0 &amp; \text{si} &amp; \mathcal{C}_i = 1
\end{array}
\right.\]</span></p>
<p>El promedio de <span class="math inline">\(s(i)\)</span> en todos las observaciones de un cluster es una medida de qué tan compactamente armado está el grupo. El promedio <span class="math inline">\(s(i)\)</span> sobre todos los datos, es una medida de qué tan buena es la clusterización.</p>
<hr />
<p>Estas notas se basan los siguientes libros:</p>
<ul>
<li><em>Introducción al análisis multivariado</em> de Jorge Blanco</li>
<li><em>Análisis de datos multivariantes</em> de Daniel Peña</li>
<li><em>An introduction to statistical learning</em> de Gareth, Witten, Hastie y Tibshirani</li>
<li><em>Multivariate statistical inference and applications</em> de Alvin Rencher</li>
</ul>
<p>Si te fueron de utilidad, podes encontrar la versión pdf de las mismas, y notas para otros temas de aprendizaje estadístico e inferencia multivariada en los siguientes links:</p>
<ul>
<li><a href="https://github.com/daczarne/udelar_analisis_multivariado_1/blob/master/01.%20Clusters/Cluster.pdf">Análisis de Clusters</a></li>
<li><a href="https://github.com/daczarne/udelar_analisis_multivariado_1/blob/master/02.%20Inferencia%20Multivariada/Inferencia-Multivariada.pdf">Inferencia</a></li>
<li><a href="https://github.com/daczarne/udelar_analisis_multivariado_1/blob/master/03.%20An%C3%A1lisis%20Discriminante/Discriminante.pdf">Análisis Discriminante</a></li>
<li><a href="https://github.com/daczarne/udelar_analisis_multivariado_1/blob/master/04.%20An%C3%A1lisis%20Factorial/An%C3%A1lisis-factorial.pdf">Análisis Factorial</a></li>
</ul>
</div>
</div>
